{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Reading LH5 Data at Scale\n",
    "\n",
    "Reading very large quantities of data can incur performance penalties if not done carefully, due to large memory usage and/or a large number of files to be read. To assist in reading large quantities of data in a scalable way, use the [LH5Iterator](https://legend-pydataobj.readthedocs.io/en/stable/api/lgdo.lh5.html#module-lgdo.lh5.iterator). This will break your data into smaller chunks so that you can process each chunk individually; in addition, special tools exist for parallelizing your processing across the chunks (parallel processing must be independent across threads!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## LH5Iterator\n",
    "\n",
    "Let's start by downloading a small test LH5 file with the [pylegendtestdata](https://pypi.org/project/pylegendtestdata/) package (note: by design, this test data is small; while this does not showcase the performance benefits of the LH5Iterator, the construction and operation of the iterator is identical, and can be scaled to much larger datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from legendtestdata import LegendTestData\n",
    "\n",
    "from lgdo.lh5 import LH5Iterator\n",
    "\n",
    "ldata = LegendTestData()\n",
    "\n",
    "# directories and channels we will be reading from...\n",
    "lh5_hit_dir = ldata.get_path(\"lh5/prod-ref-l200/generated/tier/hit/cal/p03/r001/\")\n",
    "lh5_dsp_dir = ldata.get_path(\"lh5/prod-ref-l200/generated/tier/dsp/cal/p03/r001/\")\n",
    "channels = [\"ch1084803\", \"ch1084804\", \"ch1121600\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Now, create an LH5 iterator and loop over its contents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh5_it = LH5Iterator(\n",
    "    lh5_files=f\"{lh5_hit_dir}/*.lh5\",  # all files in directory\n",
    "    groups=[f\"{ch}/hit\" for ch in channels],  # hit table for all channels\n",
    "    field_mask=[\"is_valid_cal\", \"cuspEmax_ctc_cal\"],  # read only these fields\n",
    "    buffer_len=20,  # read 20 entries at a time; default reads 100 MB worth of entries\n",
    ")\n",
    "\n",
    "# Loop over each chunk, and show the tables\n",
    "pd.set_option(\"display.max_rows\", 6)\n",
    "for tb in lh5_it:\n",
    "    display(tb.view_as(\"pd\"))\n",
    "pd.reset_option(\"display.max_rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "To really take advantage of the iterator, you can perform operations for each iteration, typically with the aim of performing some data reduction. We will cut entries in our tables with less than 500 keV of energy, and that are invalid.\n",
    "\n",
    "In addition, the iterator will return certain special values listed in the documentation; we will get the file name using ``current_files`` and the group name using ``current_groups``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "for tb in lh5_it:\n",
    "    df = tb.view_as(\"pd\")\n",
    "    df[\"file\"] = lh5_it.current_files\n",
    "    df[\"group\"] = lh5_it.current_groups\n",
    "    df = df.query(\"is_valid_cal and cuspEmax_ctc_cal>500\")\n",
    "    outputs += [df]\n",
    "df = pd.concat(outputs)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Joining multiple data sources\n",
    "Sometimes we want to pull data from multiple sources. This can be tables that we want to join together, or metadata about the data in each lh5 group.\n",
    "\n",
    "#### Friends:\n",
    "If you want to combine multiple datasets from different tiers of processing, you can create two iterators and \"friend\" them together so that as we iterate through the data, we join data from the tables in each one together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct two iterators, with the second one a \"friend\" of the first\n",
    "lh5_it = LH5Iterator(\n",
    "    lh5_files=f\"{lh5_hit_dir}/*.lh5\",  # all files in hit tier directory\n",
    "    groups=[f\"{ch}/hit\" for ch in channels],  # hit table for all channels\n",
    "    field_mask=[\"is_valid_cal\", \"cuspEmax_ctc_cal\"],  # read only these fields\n",
    "    buffer_len=20,  # read 20 entries at a time; default reads 100 MB worth of entries\n",
    "    friend=LH5Iterator(\n",
    "        lh5_files=f\"{lh5_dsp_dir}/*.lh5\",  # all files in dsp tier directory\n",
    "        groups=[f\"{ch}/dsp\" for ch in channels],  # dsp table for all channels\n",
    "        field_mask=[\"tp_0_est\", \"tp_90\"],\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Display events from both data tiers, with long drift times\n",
    "outputs = []\n",
    "for tb in lh5_it:\n",
    "    df = tb.view_as(\"pd\")\n",
    "    df = df.query(\"is_valid_cal and (tp_90 - tp_0_est)>1000\")\n",
    "    outputs += [df]\n",
    "df = pd.concat(outputs)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "#### Group data:\n",
    "We can also insert data about the groups that we are reading, which will be repeated for all data from the same group. This is useful for grabbing metadata about different channels and attaching it to events for easier analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh5_hit_dir = ldata.get_path(\"lh5/prod-ref-l200/generated/tier/hit/cal/p03/r001/\")\n",
    "channels = [\"ch1084803\", \"ch1084804\", \"ch1121600\"]\n",
    "chan_data = {\n",
    "    \"chanid\": [1084803, 1084804, 1121600],\n",
    "    \"detname\": [\"huey\", \"dewey\", \"louie\"],\n",
    "}\n",
    "\n",
    "# set group_data with our channel metadata\n",
    "lh5_it = LH5Iterator(\n",
    "    lh5_files=f\"{lh5_hit_dir}/*.lh5\",  # all files in hit tier directory\n",
    "    groups=[f\"{ch}/hit\" for ch in channels],  # hit table for all channels\n",
    "    field_mask=[\"is_valid_cal\", \"cuspEmax_ctc_cal\"],  # read only these fields\n",
    "    group_data=chan_data,  # attach our channel metadata\n",
    "    buffer_len=20,  # read 20 entries at a time; default reads 100 MB worth of entries\n",
    ")\n",
    "\n",
    "# We're going to be fancy with our loop this time...\n",
    "outputs = []\n",
    "for tb in lh5_it:\n",
    "    df = tb.view_as(\"pd\")\n",
    "    df = df.query(\"is_valid_cal and cuspEmax_ctc_cal>500\")\n",
    "    outputs += [df]\n",
    "df = pd.concat(outputs)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Querying data\n",
    "In the above examples, we have used the iterator to loop through many datasets, and select entries to put into a smaller dataset. Because this is one of the most common use-cases for `LH5Iterator`, specialized functions for speeding this process up exist.\n",
    "\n",
    "#### query\n",
    "The [query](https://legend-pydataobj.readthedocs.io/en/stable/api/lgdo.lh5.html#lgdo.lh5.iterator.LH5Iterator.query) function can be used to grab a subset of data based on some selection. The section uses [Table.eval](https://legend-pydataobj.readthedocs.io/en/stable/api/lgdo.types.html#lgdo.types.table.Table.eval) to evaluate the selection, and can access commands from `numexpr` and `awkward` packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "lh5_it = LH5Iterator(\n",
    "    lh5_files=f\"{lh5_hit_dir}/*.lh5\",  # all files in directory\n",
    "    groups=[f\"{ch}/hit\" for ch in channels],  # hit table for all channels\n",
    "    field_mask=[\"is_valid_cal\", \"cuspEmax_ctc_cal\"],  # read only these fields\n",
    "    buffer_len=20,  # read 20 entries at a time; default reads 100 MB worth of entries\n",
    ")\n",
    "df = lh5_it.query(\"is_valid_cal & (cuspEmax_ctc_cal>500)\", library=\"pd\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "For large datasets, you can also apply multi-processing to query using the ``processes`` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lh5_it.query(\"is_valid_cal & (cuspEmax_ctc_cal>500)\", processes=3, library=\"pd\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "In addition, it is possible to query data and fill a [Hist](https://hist.readthedocs.io/en/latest/) by using the [hist](https://legend-pydataobj.readthedocs.io/en/stable/api/lgdo.lh5.html#lgdo.lh5.iterator.LH5Iterator.hist) command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hist import axis\n",
    "\n",
    "h = lh5_it.hist(\n",
    "    axis.Regular(30, 0, 3000, label=\"Energy (keV)\"),\n",
    "    where=\"is_valid_cal\",\n",
    "    keys=\"cuspEmax_ctc_cal\",\n",
    ")\n",
    "display(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Both hist and query are also able to take a python function rather than a string for `where`. The function should take a table and iterator as inputs, and return a table, awkward array or pandas dataframe; this allows for more complex selection logic and for producing outputs that aren't directly in the datasets. When writing this function, don't forget to use collective operations that operate on full columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select valid events and return energy and channel name\n",
    "def selector(lh5_tb, lh5_it):\n",
    "    df = lh5_tb.view_as(\"pd\")\n",
    "    mask = df.is_valid_cal\n",
    "    E_col = df.cuspEmax_ctc_cal[mask]\n",
    "    chan_col = lh5_it.current_groups[mask]\n",
    "    chan_col = np.strings.replace(chan_col, \"/hit\", \"\")\n",
    "\n",
    "    return E_col, chan_col\n",
    "\n",
    "\n",
    "# 2D histogram with a categorical axis\n",
    "h = lh5_it.hist(\n",
    "    [\n",
    "        axis.Regular(30, 0, 3000, label=\"Energy (keV)\"),\n",
    "        axis.StrCategory([], growth=True, label=\"Channel\"),\n",
    "    ],\n",
    "    where=selector,\n",
    ")\n",
    "h.plot1d();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "### Parallel processing\n",
    "To boost the speed of accessing a large amount of data via the iterator, you can access and process datasets in parallel by calling [map](https://legend-pydataobj.readthedocs.io/en/stable/api/lgdo.lh5.html#lgdo.lh5.iterator.LH5Iterator.map). `map` works by splitting the datasets in each file and group among different threads, and iterating over them in parallel. This is built on the python standard library's [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html) `Executor` framework; the default is to use multi-processing. Both query and hist are capable of running in parallel by taking advantage of this function; `map` is intended to give advanced users more flexibility.\n",
    "\n",
    "Map will call a single function on each sub-table read while iterating through datasets. The output of the function will by default be collected into a python list; however, you can also provide an aggregator function that will combine the outputs from different tables in a customizable way (see the `aggregate` and `init` arguments). In addition, it is possible to define a function call to run before beginning and upon termination of the loop, which can be used to initialize more complex processes (see the `begin` and `terminate` arguments).\n",
    "\n",
    "Map will asynchronously run each loop, and the results (either a list or the result of aggregation) will be returned as an asynchronous iterator over these objects; this will preserve the order of the data as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count events in each channel with >400 keV of energy and store the results in a dict\n",
    "def fill(lh5_tab, lh5_it):\n",
    "    cts = {}\n",
    "    for chan in np.unique(lh5_it.current_groups):\n",
    "        key = chan.replace(\"/hit\", \"\")\n",
    "        cts[key] = np.sum(\n",
    "            lh5_tab.is_valid_cal.nda\n",
    "            & (lh5_tab.cuspEmax_ctc_cal.nda > 400)\n",
    "            & (lh5_it.current_groups == chan)\n",
    "        )\n",
    "    return cts\n",
    "\n",
    "\n",
    "# add results for each channel\n",
    "def aggregate(cts_sum, cts_in):\n",
    "    for key, cts in cts_in.items():\n",
    "        if key in cts_sum:\n",
    "            cts_sum[key] += cts\n",
    "        else:\n",
    "            cts_sum[key] = cts\n",
    "    return cts_sum\n",
    "\n",
    "\n",
    "# call map with 3 processes\n",
    "results = lh5_it.map(\n",
    "    fill,  # process chunks of data\n",
    "    aggregate=aggregate,  # function for combining results\n",
    "    init={},  # initial value of aggregation\n",
    "    processes=3,\n",
    ")\n",
    "\n",
    "# Now we want to aggregate the results from each thread\n",
    "total_cts = {}\n",
    "for cts_sum in results:\n",
    "    aggregate(total_cts, cts_sum)\n",
    "\n",
    "display(total_cts)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
